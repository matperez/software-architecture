# Summarized System Design Problems
## Сервис сокращения ссылок типа TinyURL
### 1. Зачем нам нужна сокращалка ссылок?
Сокращалка ссылок создает короткие алиасы для длинных ссылок. Мы отдаем ей длинную ссылку, получаем в ответ короткую и отправляем ее другу. Друг переходит по короткой ссылке и получает редирект на исходный адрес.

Сокращалки ссылок используются для оптимизации обмена между устройствами, трекингом исходных ссылок для целей аналитики или скрытия исходных адресов.

### 2. Требования и цели системы
Сокращалка ссылок должна удовлетворять следующим требованиям

**Функциональные требования:**
- Получив ссылку, наш сервис должен сгенериротьва короткий уникальный алиас для нее. Это называется короткой ссылкой. Эта ссылка должна быть достаточно короткой, чтобы ее можно было легко скопировать и вставить в приложение.
- Когда пользователь переходит по короткой ссылке, наш сервис должен редиректнуть его на исходный адрес.
- Пользователь должен иметь возможность при желании выбрать собственный текст для короткой ссылки.
- Ссылка должна устаревать через некоторое время. Пользователь может выбрать какое время будет актуальна ссылка.

**Нефункциональные требования:**
- Система должна иметь высокую надежность. Это требуется поскольку при падении нашей системы, короткие ссылки перестанут работать.
- Редирект на исходный адрес должен происходить в реальном времени с минимальной задержкой.
- Сокращенные ссылки должны быть непредстазуемы, неугадываемы.

**Дополнительные требования:**
- Нужна аналитика (сколько раз происходили редиректы, откуда приходили пользователи).
- Сервис так же должен быть доступен через REST API для других сервисов.

### 3. Оценка емкости и ограничений
Наша система будет нагружена по чтению. Запросов на кароткие урлы будет больше, чем на их сокращение. Предположим, что соотношение будет 100:1

**Оценка количества запросов:** Предположим мы имеет 500 млн новых урлов в месяц. С коэфф. 100:1 мы молучим 50 млрд. чтений за тот же период.

Какое будет число запросов (RPS) для нашей системы на запись? 

500 млн / (30 дней * 24 часа * 3600 сек) = ~200 URLs/s

С коэфф 100:1 это будет 100 * 200 = 20 К/s записей.

**Оценка хранилища:** Допустим мы сохраняем каждый URL на 5 лет. Поскольку мы ожидаем 500M новых урлов каждый месяц, то общее количество записей в базе будет 30 млрд:

500M * 5Y * 12M = 30 B

Предположим для начала, что каждая запись будет занимать 500 байт (мы уточним эту цифру позже), тогда нам понадобится 15 TB дисквого пространства:

30 billinons * 500bytes = 15 TB

**Оценка трафика:** Для запросов на запись, поскольку мы ожидает 200 записей каждую секунду, мы получим 100KB/s:

200 * 500 bytes = 100KB/s

Для запросов на чтение трафик составит 10 MB/s:

20K * 500 bytes = 10 MB/s

**Оценка памяти:**  Если мы хотим кешировать ссылки, которые чаще всего запрашивают пользователи, то сколько памяти нам нужно для этого? Допустим 20% ссылок генерируют 80% трафика. Сколько памяти нам нужно на эти 20% ссылок?

Раз у нас есть 20K запросов в секунду, мы получим 1.7B запросво в день:

20K * 3600 sec * 24 hour = 1.7B

Чтобы закешировать 20% от этих запросов нам нужно 170 GB памяти:

0.2 * 1.7B * 500 bytes = 170 GB

Поскольку у нас будет много запросов на тот же самый URL, реальные затраты памяти будут ниже, чем 170 GB.

**Итого:** Предполагая 500 млн новых урлов в месяц и коэфф чтения к записи 100:1, мы получим следующие оценки:

![](attachments/Pasted%20image%2020220510143035.png)

### 4. API системы
Мы можем использовать SOAP или REST API для взаимодействия с нашим сервисом. Следующие методы могут использоваться для создания и удаления ссылок.

```
createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None)
```

api_dev_key можно использовать для зарегистрированных аккаунтов для троттлинга запросов согласно выделенной квоты.

```
deleteURL(api_dev_key, url_key)
```

url_key - это сокращенный URL

**Как нам определить и предотвратить злонамеренное использование?** Установить лимиты создания и получения ссылок на каждого отдельного разработчика. А если сервис позволяет анонимный доступ? Тогда можно сделать лимиты на создание с конкретного IP и лимиты на чтение на конкретный IP и на отдельные ссылки.

### 5. Дизайн БД, модель данных
Некоторые наблюдения относительно данных, которые нам нужно будет хранить:
- Нам нужно хранить миллиарды строк
- Каждый объект, которым мы храним будет небольшим (меньше 1K)
- Между записями нет связи (кроме того, какой пользователь их создал)
- Наш сервис нагружен по чтению

Нам может понадобиться две базы: одна для хранения информации об URL и другая для хранения информации о пользователях.

![](attachments/Pasted%20image%2020220510150111.png)

**Какую БД мы должны использовать?** NoSQL хранилище типа [DynamoDB](https://en.wikipedia.org/wiki/Amazon_DynamoDB), [Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) или [Riak](https://en.wikipedia.org/wiki/Riak) будет наилучшим выбором. Такое хранилищие к тому же будет проще масштабировать.

### 6. Базовый дизайн системы и алгоритмы
Нам нужно определиться каким образом мы будем генерировать короткую форму для данной ссылки.

В примере с TinyURL сокращенные ссылки выглядят так [https://tinyurl.com/rxcsyr3r](https://tinyurl.com/rxcsyr3r). Последние 8 символов в ссылке - это и есть тот ключ, который мы хотим сгененрировать. 

Мы рассмотрим два решения.

#### a. Кодирование исходного URL
Мы можем считать уникальный хэш (MD5 или SHA256) от переданной ссылки. Хэш можно заенкодить для отображения: base36 ([a-z,0-9]) или base62([A-Z,a-z,0-9]), а если добавить еще знаки + и /, то получим base64. Резонным вопросом тут будет, какой длины должен быть короткий ключ? 6, 8 или 10 символов?

- Используя кодировку base64 и ключ длинно 6 символов мы получим 64^6 = 68 млрд возможных строк.
- Для ключа длинной 8 символов получим уже 64^8 - 280 триллионов строк.

В принципе, нам достаточно 68B строку, так что предположим, что ключ будет состоять из 6 символов.

Алгоритм MD5 генерирует 128-битный хеш. После кодирования в BASE64 мы получим строку более 21 символа в длину (т.к. каждый символ в base64 кодирует 6 бит)

Как нам выбрать ключ, если на него у нас есть только от 6 до 8 символов? 

Можно выбрать только часть хеша для формирования ключа. Это может привести к появлению дублей. Можно попробовать брать символы для ключа из разных мест строки или менять их местами (это в любом случае выглядит как костыль).

А есть ли еще минусы у такого решения?
- Если несколько пользователей добавят один и тот же URL - они получат один и тот же ключ, что может быть неприемлемо.
- Если часть ключа будет URL-encoded, то такая же ссылка, но не заэнкоженная будут давать разные ключи.

Как вариант, мы можем добавлять некое монотонно возрастающее значение к каждому из кодируемых урлов. Нам не обязательно сохранять это значение в базе, оно нам нужно просто, чтобы делать каждую ссылку уникальной. Проблема тут будет в том как в принципе получить моотонно возрастающую последовательность числел. Должны ли они все быть уникальными? Могут ли они пересекаться?

Другим решением может быть добавление ID пользвоателя к строки перед кодированием, но это работает только зарегистрированных пользователей. Для незарегистрированных мы можем попробовать сгенерировать пользователю свой уникальный ключ на данную конкретную сессию.

![](attachments/Pasted%20image%2020220510153541.png)

#### b. Генерация ключей оффлайн
Мы можем завести отдельный сервис для генерации ключей (Key Generation Service), который бы генерировал уникальные случайные ключи и складывал их в базу (key-DB). Каждый раз, когда нам нужно добавить новую ссылку, мы можешь пойти в базу и взять оттуда новый ключ. Просто и быстро. Нам не нужно кодировать ссылки или заботиться об уникальности ключей, так как сервис генерации ключей делает это за нас.

**Может ли конкуррентность стать проблемой?** Как только ключ использован, мы должны пометить его в базе, чтобы никто другой не смог его использовать повторно. Если несколько северов получают ключи из базы конкуррентно может возникнуть ситуация, когда сразу несколько из них получат один и тот же ключ.

Сервера могут использовать KGS чтобы получать/отмечать ключи в БД. KGS может хранить ключи в двух базах: в одной ключи, которые еще не были использованы, в другой ключ, которые уже использовались. Как только KGS отдает ключ кому-то, он должен переносить его из первой таблицы во вторую.

KGS может хранить некототорое количество ключей в памяти, чтобы выдать их как можно быстрее. Для простоты реализации, KGS может сразу переносить ключ в использованные как только он загрузил его из базы. Если он умрет, мы потеряем часть ключей, но эта потеря будет не критична.

KGS должен следить за тем, чтобы не выдавать один и тот же ключ нескольким серверам. Для этого нужно синхронизировать доступ или ввести какой-то вариант локов на структуру, которая хранит ключи.

**Какой будет размер у базы для ключей?** В кодировкой base64, мы можем сгенерировать 68B значений ключа из 6 символов. Если на 1 символ будет уходить 1 байт, нам потребуется:

6 символов * 68 млрд. ключей = 412 GB

**Является ли KGS единой точкой отказа?** Конечно является. Мы можем иметь стендбай реплику сервиса, чтобы при падении основного сервера его место занял резервный.

**Могут ли сервера приложений кешировать ключи, полученные из key-DB?** Могут и это повысит производительность. Но в этом случае при падении сервера мы потеряем определенное количество ключей. Поскольку у нас ключей с запасом, мы можем себе это позволить.

**Как нам выполнять поиск ключей?** Мы можем воспользоваться механизмами протокола HTTP. Если ключ найден, отдаем 302 редиррект на нужную страницу. Если ключа нет, отдаем 404 или редиректим пользователя на главную страницу сервиса.

**Нужно ли нам задавать ограничения на кастомные ключи?** Да, мы можем их задать. Например можно ограничить длину кастомного ключа 16 символами.

![](attachments/Pasted%20image%2020220510155843.png)

### 7. Партицирование и репликация
Наша база скорее всего не взелет за один сервер, для масштабирования ее на требуемые объемы и производительность нам нужно партицировать ее на несколько серверов. Нужно выбрать схему по которые мы будет ее партицировать.

**a. Range-Based Partitioning:** Мы можем партицировать базу на основе первой буквы ключа. Например все что начинается на **a** пойдет на один сервер, все что начинается на **b** на другой т.д. Этот подход называется Range-Based партицирование. Мы также можем объединять некоторые редкие сочетания символов в одну партицию.

Основаня проблема в этом подходе в том, что может возникнуть дизбаланс между серверами, если какие-то из символов в ключах встречаются сильно чаще или реже остальных.

**b. Hash-Based Partitioning:** В этом подходе мы берем хеш от объекта, который хотим сохранить и вычисляем хранилище на основе этого хеша. Хеширование позволяет распределить данные по партициям более равномерно. Например, наша функция хеширования всегда может возвращать рандомное значение от 1 до N, где N - общее количество серверов.

Этот подходит все еще может приводить к дизбалансу, который решается применением [Консистентного Хеширования](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5709068098338816/)

### 8. Кеширование
Мы можем кешировать ссылки, которые часто запрашиваются. Можно использовать любое готовое решение типа Memchached, которое способно хранить ключи и соответствующие им ссылки. Сервер приложения может заглядывать в кеш за данными прежде чем искать ссылку у себя в хранилище.

**Сколько памяти нам нужно отвести под кеш?** Можно начать с 20% дневного трафика, а потом поправить это значение на основе опыта использования. Выше мы посчитали, что нам требуется 170 GB памяти, чтобы кешировать 20% дневного трафика. Современные сервера держат до 256 GB ОЗУ, так что можно легко уместить кеш на одном сервере. В качестве альтернативы можно использовать несколько серверов для кеширования.

**Какую стратегию вытеснени ключей нам стоит использовать?** Когда кеш полон и мы хотим добавить новую запись в него, как нам определить что можно выкинуть, а что стоит оставить? Можно использовать политику LRU (Least Recently Used). 

Для дальнейшего увеличения эффективности мы можем реплицировать наши кеширующие сервера для распределения нагрузки на них.

**Как мы будем обновлять реплики?** Каждый раз, когда данные отсутствуют в кеше, сервера приложений будут обращаться в базе данных. Каждый раз, когда это происходит, мы можем обновлять кеш на всех репликах. Если реплика уже имеет такую запись, она может игнорировать обновление.

![](attachments/Pasted%20image%2020220510163540.png)

### 9. Балансировка нагрузки (LB)
Мы можем добавить слой балансировки нагрузки в трех разных местах нашей системы:
- Между клиентом и серами приложений
- Между серверами приложений и базой данных
- Между серверами приложений и кешем

Для начала можно использовать равномерное распределение через Round Robin. Такая балансировка проста и эффективна в реализации. Если сервер умрет, его можно исключить из распределения.

Проблема может быть в том, что RR не учитывает нагрузку серверов. Как результат если сервер перегружен или работает медленно запросы могут его убить. Чтобы это обработать можно внедрить какое-нибудь более интеллектуальное решение, которое учитывает текущую нагрузку серверов.

### 10. Очистка БД
Должны ли записи в БД храниться бесконечно или их нужно удлять время от времени? Что должно происходить, когда время хранения ссылки истекает?

Если мы начнем постоянно искать и удалять протухшие ссылки, это создаст большую нагрузку на БД. Вместо этого мы можем медленно и лениво чистить базу. Наш сервис не будет отдавать протухшие ссылки, хотя они и могут оставать в базе дольше положенного.

- Каждый раз, когда пользователь обращается к протухней ссылке, мы можем возвращать ему ошибку и именно в этот момент удалять ссылку.
- Отдельный сервис может чистить базу в моменты минимальной нагрузки на нее.
- Можно указать дефолтное время жизни для ссылок, чтобы они точно не оставались в базе навечно.
- После удаления протухней ссылки, можно возвращать ее ключ обратно в key-DB.

![](attachments/Pasted%20image%2020220510165108.png)

### 11. Телеметрия
Скорее всего мы захотим знать сколько раз запросили тот или иной URL, откуда пришли посетители. Как нам хранить эту статистику? Если это будет часть строки в БД, которая хранит данные не загнемся ли мы с большим числом запросов на какой-то из популярных урлов?

### 12. Безопасность и ограничение доступа
Могут ли пользователи создавать приватные ссылки или ограничивать доступ к ссылкам для конкретных пользователей?

Мы можем сохранять уровень доступа (приватный/публичный) с каждой ссылкой в БД. Так же мы можем создать отдельную базу для хранения ID пользователей, которые имеют доступ к данной ссылке.

Если у пользователя нет доступа, можно отдавать ему HTTP-код 401.

## Сервис передачи коротких текстов Pastebin
Тут мы задизайним сервис типа Pasebin через который пользователи могут обмениваться фрагментами текста. Пользователи отправляют фрагмент текста и получают рандомно генерируемый URL для доступа к нему.

### 1. Что такое Pastebin?
Pastbin - это сервис в котором пользователи могут сохранять отрывки текста или картинки по сети и получать в ответ короткий URL для доступа к загруженным данным.

### 2. Требования и цели системы
**Функциональные требования:**
- Пользователи должны возможность загружать свои данные и получать уникальный URL для доступа к ним.
- Пользователи должны иметь возможность только загружать текст
- Данные и ссылки протухают через некоторое время. Пользователи должны иметь возможность задать собственное время жизни для контента.
- Пользователи должны иметь возможность задать собственный алиас для загруженного контента.

**Нефункциональные требования:**
- Система должна быть надежной, загруженные данные не должны теряться
- Система должна быть доступной, если она упадет, пользователи потеряют доступ к своему контенту.
- Данные должны быть доступны в реальном времени с минимальной задержкой.
- Ссылки не должны угадываться (быть предсказуемыми)

**Дополнительные требования:**
- Нужна аналитика (кто откуда и как часто имеет доступ к сервису)
- Нужен доступ через REST API для других сервисов

### 3. Некоторые аспекты дизайна
Pastebin очень похож на сервис сокращения ссылок. Но тут есть некоторые отличия, которые нужно держать в уме.

**Какой должен быть лимит загружаемых данных?** Мы можем ограничить размер одной записи скажем 10 МБ.

**Нужно ли ограничивать длину кастомных URL?** Пользователи могут задавать свои псевдонимы, но нет смысла делать их слишком длинными. Для единообразия можно ограничить их длину разумной границей. Сверху наверное это будет общая длинна ссылки 255 символов, но пусть у нас будет 20 символов.

### 4. Оценка емкости и ограничений
Сервис будет нагружен по чтению. Можем предположить что соотношение чтений к записи будет 5:1.

**Количество запросов:** Pastebin вряд ли будет также популярен как Twitter или Facebook. Предположим, что будет где-то миллион новых Паст каждый день. Это означает примерно 5M чтений Паст каждый день.

Новых Паст в секунду: 1M/(24 hours * 3600 sec) = 12 pastes/sec

Чтений Паст в секунду: 12 * 5 = 58 pastes/sec

**Хранилище:** Пользователи могут загружать до 10 MB данных за раз, но обычно грузят исходнико кода, конфиги, логи. Таким тексты занимают не так много места. Предположим, что в среднем запись будет занимать около 10 KB.

С такими оценками мы будем загружать около 10 GB данных в день: 1M * 10KB = 10 GB/day.

С 1M записей в день, за 10 лет мы накопим  3.6 B записей. Нам нужно генерировать уникальный ключ для них. Если использовать base64 ([A-Z, a-z, 0-9, ., -]) для кодирования ключа, нас устроит длина в 6 символов: 64^6 = 68 B уникальных строк.

Если на каждый символ ключа мы затратим 1 байт, то для всех ключей нам понадобится: 3.6 B * 6 bytes = 22 GB. Это пренебрежительно мало по сравнении с местом требуемым под данные.

За 10 лет накопится 36 TB данных. Нам стоит оставить запас в хранилище, чтобы оно ни при каких условиях не было заполнено больше, чем на 70%, таким образом для хранение данных нам нужно будет хранилище в 51.5 TB.

**Трафик:** Для запросов на запись мы ожидаем 12 RPS, итого 12 * 10KB = 120 KB/s

Для запросов на чтение мы ожидаем 58 RPS, так получим 58 * 10KB = 600 KB/s

**Оперативная память:** Мы можем кешировать наиболее популярые Пасты. Возьмем примерное распределение 20/80 и будем кешировать 20% запросов на чтение.

Поскольку у нас 5M запросов на чтение в день, нам понадобится 0.2 * 5M * 10KB = 10 GB памяти.

### 5. API системы
Сервис можно предоставлять через SOAP или REST. Апишка может выглядеть примерно так.

```
addPaste(api_dev_key, paste_data, custom_url=None, user_name=None, paste_name=None, expire_date=None)

getPaste(api_dev_key, api_paste_key)

deletePaste(api_dev_key, api_paste_key)
```

### 6. Дизайн БД
- Нам нужно хранить миллиарды записей
- Метаданные невелики (меньше 1К на запись)
- Каждая Паста может быть среднего размера (несколько MB)
- Между записями нет отношений, кроме записей о пользователе, который создал запись.
- Сервис нагружен по чтению

Для хранения данных нам может понадобиться 2 таблицы: для хранения самих данных и для хранения данных о пользователях

![](attachments/Pasted%20image%2020220510202220.png)

Тут URLHash - это ссылка аналогичная оной в сокращалке ссылок, а ContenKey - это ссылка на объект во внешне хранилище.

### 7. Общий дизайн
На высоком уровне нам понадобится уровень приложения для обработки запросов пользователя и уровень хранения данных к которым будет обращаться сервер приложений. Мы можем разделить базы на отдельное хранилище для метаданных и отдельное объектное хранилище типа Amazon S3 для хранения содержимого Паст. Это позволит нам масштабировать их независимо.

![](attachments/Pasted%20image%2020220510202546.png)

### 8. Дизайн компонентов
#### a. Уровень приложения
Уровень приложения должно обрабатывать все входящие и исходящие запросы. Сервер приложений взаимодействует с бекендом хранения.

**Как обрабатывать запросы на запись?** Получив запрос на запись сервер приложения должен сгенерировать 6-символьную случайную строку которая будет являться ключем для доступа к данных (исли пользователь не предоставил свой ключ). После этого он должен сохранить содержимое пасты и сгенерированный ключ в БД. После успешного сохранения сервер должен вернуть ключ пользователю.

Одной из проблем тут может быть то, что сгенерированный ключ уже находится в базе. В этом случае мы должны сгенерировать новый ключ и попробовать сохранить данные еще раз, и делать так до тех пор пока данные не удастся сохранить нормально. Если же в базе существует кастомный ключ пользователя, то тут уж останется только вернуть ошибку.

Другим решением для этой проблемы может быть использование отдельного сервиса генерации ключей (Key Generation Service aka KGS), который должен генерировать уникальные ключи заранее и сохранять их в базу ключей (key-DB). Суть такого сервиса подробно описана в описании сервиса сокращения ссылок.

**Каким образом обрабатывается запрос на чтение?** Получив запрос на чтение, сервис приложения обращется к хранилищу данных. Хранилище ищет ключ и, если находит, возващает содержимое Пасты.

#### b. Уровень хранения данных
Мы можем разделить уровень хранения на две части:
- Хранилище метаданных. Мы можем использовать реляционную базу типа MySQL или распределенное KV-хранилище типа Dynamo или Cassandra.
- Объектное хранилище. Мы можем сохранять контент в Объектном хранилище типа Amazon S3. Каждый раз, когда мы будет приближаться к лимиту хранилища, его можно будет легко расширить.
 
![](attachments/Pasted%20image%2020220510205325.png)

### 9. Очистка БД
### 10. Партицирование и репликация
### 11. Кеширование и балансировка нагрузки
### 12. Безопасность и права доступа
В этом плане все тоже самое, что и в сервисе сокращения ссылок. Можно почитать там.

## Instagram
Сервис публикации фото типа Instagram, где пользователи могут загружать фотографии и делиться ими с окружающими.

### 1. Что такое Instagram?
Instagram - это социальная сеть, которая позволяет пользователям делиться своими фото и видео с другими пользователями. Пользователь может выбрать делиться информацией публично или приватно. Все чем поделилилсь публично может быть увидено другими пользователями, все приватое доступно только ограниченному кругу лиц. Так же Instagram позволяет делиться своими записями через другие социальные сети типа Facebook, Twitter, Flickr и т.п.

Мы попробуем задизайнить более простую версию Instagram где пользователь может шарить фото и фолловить других пользователей. Лента новостей для каждого пользователя будет состоять из самых лучших фоток других пользователей на которых он подписан.

### 2. Требования и цели системы
**Функциональные требования:**
- Пользователи должны иметь возможность загружать, скачивать, просматривать картинки
- Пользователи могут искать картинки/видео по названию
- Пользователи могут подписываться на других пользователей
- Система должна генерировать и показывать ленту новостей состоящую из лучших фоток всех на кого подписан пользователь.

**Нефункциональные требования:**
- Сервис должен быть высокодоступным
- Приемлемое время отклика для генерации ленты новостей 200мс
- Допустима итоговая согласованность, если другие пользователи не увидят загруженное фото сразу - это ок.
- Система должна быть высоконадежной, потеря фото или видео недопустима.

**Не берем в реализацию:** Добавление тегов, поиск по тегам, комментирование фото, отмечание пользователей на фото, информация о том, кто кого фолловит и т.д.

### 3. Общие соображения по дизайну
Система будет нагружена по чтению, так что мы сконцентрируемся на создании системы, которая может получать и отображать фотки быстро.
- Фактически пользователи могут загружать любое количество фоток, так что эффективность управления системой хранения - это важный фактор дизайна этой системы.
- Низкое время отклика ожидается во время просмотра фоток.
- Данные должны быть 100% сохранны. Если пользователь загрузил фотку, она 100% не должна потеряться.

### 4. Оценка емкости и ограничения
- Допустим у нас 500М пользователей, из которых 1М ежедневно активны.
- 2М фото загружаются каждый день - 23 новых фотки в секунду.
- Средний размер фотографии >= 200KB
- Для хранения всех новых фоток за день требуется 2M * 200KB = 400 GB
- Для хранения всех фоток за 10 лет требуется: 400GB * 365 days * 10 years = 1425 TB

### 5. Общий дизайн
В общем виде нам потребуется поддержка двух сценариев:
- Загрузка фото
- Просмотр фото
Наш сервис потребует объектное хранилище, что сохранения контента и некую БД для хранения метаданных.

![](attachments/Pasted%20image%2020220510232045.png)

### 6. Схема БД
Нам нужно хранить данные пользователей, их загруженные фото и людей на которых они подписаны. Таблица для фото будет хранить все данные относящиеся к фото. Нам нужен будет индекс по PhotoID, CreationDate так как мы хотим получать самые свежие фото сначала.

![](attachments/Pasted%20image%2020220510233145.png)

Прямолиненым подходом будет хранить все данные в реляционной БД типа MySQL так как нам требуются джойны. Но реляционная БД имеет и свои минусы, особенно когда нам нужно масштабировать ее. 

Мы можем хранить фото в распределенном хранилище типа HDFS или Amazon S3.

Мы можем хранить метаданные в распределенном KV-хранилище и пользоваться всеми плюшками NoSQL. Все метаданные могут храниться в таблице, где ключем будет являться PhotoID, а значением объект содержащий информацию о фото: Location, Timestamp и т.п.

NoSQL хранилища как правило имеют некоторое количество запущенных реплик, нам это будет полезно для повышения производительности и надежности. Так же в подобных хранилищах данные часто не удаляются сразу, а сохраняются на некоторое время прежде чем быть удаленными окончательно.

### 7. Оценка размера данных
Посчитаем сколько данных нам нужно будет сохранить в каждую и таблиц и сколько всего данных мы накопим за 10 лет.

**User:** Допустим мы используем 32битные целые числа для ID и Timestamp. Тогда каждая строка в базе пользователей будет занимать

UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + LastLogin (4 bytes) = 68 bytes

На 500M пользователей, нам потребуется 500M * 68 = 32GB

**Photo:** На каждое фото в таблице нам потребуется

PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotoLongitude (4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes.

Если 2М новых фоток загружается каждый день, нам нужно будет на них 2M * 284 bytes = 0.5 GB каждый день. 

За 10 лет накопится 0.5 GB * 10 years * 356 days = 1.9 TB данных.

**UserFollow:** Каждая строка таблицы UserFollow состоит из 8 байт. Если у нас есть 500M пользователей и в среднем каждый пользователь фолловит 500 других пользователей, нам потребуется: 500M * 500 followers * 8 bytes = 1.8 TB

Итого, за 10 лет мы накопим 32GB + 1.9 TB + 1.8 TB = 3.7 TB данных.

### 8. Дизайн компонентов
Загрузка фото (запись) может быть долгой, так как записть производится на диск. В то же время чтение может быть очень быстрым, особенно, если удастся отдать данные из кеша.

Загрузка данных может занять все доступные коннекшены т.к. загрузка - это долгий процесс. Это означает, что чтения не смогут отдаваться той же системой, т.к. она будет полностью занята процессом записи. Нужно держать в уме, что число коннектов к одной серверу ограничено (где-то 500 коннектов в любое время). Чтобы устранить это узкое место мы можем разделить чтение и запись по разным сервисам. 

Это разделение так же поможет нам масштабировать эти сервисы отдельно в зависимости от их требований.

![](attachments/Pasted%20image%2020220511000331.png)

### 9. Надежность и избыточность
Терять файлы непримлемо для нашего сервиса. Поэтому мы будем хранить несколько копий каждого файла. Так что, если одно из хранилищ упадет, фото можно будет достать с дублирующего.

Тот же принцип применим к другим компонентам системы. Если мы хотим иметь высокую доступность, нам нужно иметь несколько реплик наших сервисов. Избыточность страняет единую точку отказа.

Если в данном месте системы нужна только одна копия запущенного сервиса, мы может держать вторую прозапас и подключать ее только если основная реплика упадет. Переключение может происходить автоматом или вручную.

![](attachments/Pasted%20image%2020220511000720.png)

### 10. Шардирование
Обсудим различные варианты шардирования.

#### a. Партицирование по UserID
Давайте предположим, что мы делаем шарды на основе UserID, так что мы можем хранить все фото пользователя на том же шарде. Если один шард может хранить 1 TB, нам понадобится 4 таких, чтобы сохранить 3.7 TB данных. Давайте предположим, что для лучшей производительности и масштабируемости, мы будет использовать 10 шард.

Таким образом номер шарда мы будет определять как остаток от целочисленного деления на 10: UserID % 10. Чтобы уникальным образом идентифицировать фото на конкретном шарде, можно добавлять его номер к ID фото.

**Как нам генерировать ID фото?** Каждый шард БД может иметь свой собственный автоинкрементный ID для фоток. Таким образом, объединив ShardID и PhotoID мы получим ID уникальный для всей системы.

**Какие еще могут быть проблемы с такой схемой шардирования?**
- Как нам поступать с популярными пользователями? Они фолловят только нескольких пользователей, но очень многие подписываются на них.
- Некоторые пользователи загружают гораздо больше фотографий, чем остальные. Это можно привести к нерамномерной загрузке стораджей.
-  Что если у нас не получится залить все фотки на один шард? Приведет ли распределение пользовательских данных на несколько шардов к повышению время отклика при высокой нагрузке?

#### b. Партицирование по PhotoID
Если мы будем заранее генерировать ID фото, а потом брать %10 для опделения номера шарда - это позволит нам избежать проблем описанных выше, т.к. ID фото уже будет уникальным для всей системы.

**Как нам сгенерировать ID фото?** Тут мы не можем использовать автоинкрементное значение отдельных шардов, т.к. нам сначала нужно сгенерировать ID фото, чтобы узнать в какой шард оно попадет. Одним из решений может быть отдельный инстанс БД, генерирующий последовательные ID. Если ID фотки поместится в 64 бита, мы можем решить это одной таблицей, в которой будет только автоинкрементный ключ, так что при появлении нового фота мы можем вставить туда новую строку и взять ее ID.

**Будет ли эта БД генерирующая ключи единой точкой отказа?** Да, будет. Решением может быть использование двух подобных БД параллельно: одну для генерации четных ID и другую для генерации нечетных ID (чтобы они не пересекались). Для MySQL подобная настройка может быть реализована так:
```
KeyGeneratingServer1:
auto-increment-increment = 2
auto-increment-offset = 1

KeyGeneratingServer2:
auto-increment-increment = 2
auto-increment-offset = 2
```

Можно поместить перед ними балансировщик и распределять запросы по RoundRobin. Даже если какая-то БД будет временно недоступна и перестанет генерировать ID, она отстанет от другой, но это не создаст нам проблем.

Подобным же образом можно генерировать ID для пользователей, комментариев и любых других сущностей в нашей системе.

**В качестве альтернативы** мы можем генерировать ключ аналогичным образом как бы делали это в сокращалке ссылок.

**А еще** можно использовать UUID или Snowflake для генерации уникальных ID.

**Как мы можем запланировать дальнейший рост системы?** Мы можем создать больше партиций, чем необходимо и частично аггрегировать их на одном и том же сервере. Когда данных станет существенно больше, мы можем разнести логические партиции на разные сервера.

### 11. Ранжирование и генерация ленты новостей
Для создания ленты новостей для заданного пользователя нам нужно получить последние, наиболее популярные и релевантные фото пользователей на которых он подписан.

Для простоты давайте предположим, что нам нужно получить топ 100 фоток для ленты новостей пользователя. Наш сервер приложений сначала получит подписки пользователя, потом получит метаданные каждого из последних 100 фото. На последнем шаге сервис отправит все эти фото в наш алгоритм ранжирования, который определит лучшие 100 фоток (на основе свежести, количества лайков и т.д.) и вернет их пользователю.

Возможной проблемой этого процесса может быть высокая задержка поскольку нам нужно опросить несколько таблиц и выполнить определенные действия над результатами запросов. Чтобы увеличить производительность мы можем генерировать ленту новостей заранее для каждого пользователя и сохранять ее в отдельной таблице.

**Предгенерируемая лента новостей:** Мы можем иметь выделенные сервера, которые будут постоянно геенирировать ленты новостей для разных пользователей и складывать их в отдельную таблицу UserNewsFeed, так что когда какой-нибудь пользователь захочет посмотреть свою ленту новостей мы просто вытащим данные из БД одним запросом и отдадим ему.

Каждый раз, когда им нужно генерировать ленту новостей, этот сервис сможет сходить в UserNewsFeed и посмотреть когда лента обновлялась в последний раз, а после этого сгенерировать обновленную ленту по алгоритму приведенному выше.

**Каким образом лента новостей может достигать пользователей?**
- **Pull:** Клиенты могут вытягивать содержимое ленты через регулярные интерфвалы или вручную, когда им это потребуется. Пробемы такого подхода: 1. новые данные не дойдут до пользователя, пока он их явно не запросит, 2. большую часть запросы на новый контент будут отрабатывать впустую
- **Push:** Сервер может пушить новые данные пользователям как только они будут появляться. Чтобы эффективно использовать этот механизм, можно использовать механизм Long Poll  запросов. Возможной проблемой с таким походом может быть пользователи, на которых подписано слишком много подписчиком или наоборот подписчики, которые подписаны на слишком много пользователей. В этом случае пушить обновления придется слишком часто.
- **Hybrid:** Мы можем использовать гибридный подход. Мы можем переместить всех пользователей, с большим числом обновлений на pull-модель, а пушить данные только пользователям с числом подписок не превышающим несколько сотен. Другим подходом может быть пушить обновления не чаще определенной частоты, а в остальном давать пользователям самим вытягивать обновления с сервера.

### 12. Создание ленты новостей в шардированными данными
Одно из основных требований ленты новостей - получение самых последних фоток всех на кого подписан пользователей. Для этого нам нужен механизм сортировки фоток по времени их создания. Чтобы делать это эффективно мы можем сделать время создания фотки частью PhotoID. Посколько PhotoID является основным ключем, мы сможем очень быстро находить последние фотки.

Мы можем использовать unix timestamp для этого. Например, пусть PhotoID содержит две части: timestamp_unique-id. Мы можем вычислить номер шарда по этому ID (PhotoID % 10) и сохранить туда фотку.

**Каким может быть размер PhotoID?** Допустим наша метка времени начинает действовать сегодня. Сколько бит нам потребуется для хранения информации о числе прошедших секунд на следующие 50 лет?

86400 sec/day * 365 days * 50 years => 1.6 B sec

Нам может понадобиться 31 бит для хранения этого номера. Поскольку в среднем мы ожидаем 23 новых фотки в секунду, мы можем аллоцировать еще 9 бит для хранения автоикрементного ID (мы делаем это, чтобы получить целое число байт 40 bit => 5 byte). Так что каждую секунду мы можем сохранять 2^9 => 512 новых фоток. Мы можем обнулять автоикнрементный счетчик каждую секунду.

Позже обсудим эту технику в дизайне Твиттера.

### 13. Кеширование и балансировка нагрузки
Наш сервис потребует доставки фоточек глобально распределенным пользователям. Для повышения скорости отклика, стоит размещать контент как можно ближе к конечному пользователю используя географически распределенные кеширующие сервера (CDN).

Мы так же можем добавить кеш для метаданных. Можно использовать Memcached чтобы кешировать данные. В качестве политики для вытеснения ключей можно использовать LRU.

**Можем ли мы сделать более интеллектуальный кеш?** Можно поробовать кешировать 20% самых популярных фоточек расчитывая на то, что они покроют 80% трафика.


